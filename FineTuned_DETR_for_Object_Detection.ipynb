{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnOdxdgVL7Sw"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets evaluate accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OenaYziWMP8T"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpEvjmkbMP_C"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"microsoft/conditional-detr-resnet-50\"  # or \"facebook/detr-resnet-50\"\n",
        "IMAGE_SIZE = 480"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iEZ_L7VMQBy"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "cppe5 = load_dataset(\"rishitdagli/cppe-5\")\n",
        "\n",
        "if \"validation\" not in cppe5:\n",
        "    split = cppe5[\"train\"].train_test_split(0.15, seed=1337)\n",
        "    cppe5[\"train\"] = split[\"train\"]\n",
        "    cppe5[\"validation\"] = split[\"test\"]\n",
        "\n",
        "cppe5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcO4ZasNMQEX"
      },
      "outputs": [],
      "source": [
        "cppe5[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JgA7uOxMQHB"
      },
      "outputs": [],
      "source": [
        "categories = cppe5[\"train\"].features[\"objects\"][\"category\"].feature.names\n",
        "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77-sgC31MQJ1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "image = cppe5[\"train\"][31][\"image\"]\n",
        "annotations = cppe5[\"train\"][31][\"objects\"]\n",
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "for i in range(len(annotations[\"id\"])):\n",
        "    box = annotations[\"bbox\"][i]\n",
        "    class_idx = annotations[\"category\"][i]\n",
        "    x, y, w, h = tuple(box)\n",
        "    # Check if coordinates are normalized or not\n",
        "    if max(box) > 1.0:\n",
        "        # Coordinates are un-normalized, no need to re-scale them\n",
        "        x1, y1 = int(x), int(y)\n",
        "        x2, y2 = int(x + w), int(y + h)\n",
        "    else:\n",
        "        # Coordinates are normalized, re-scale them\n",
        "        x1 = int(x * width)\n",
        "        y1 = int(y * height)\n",
        "        x2 = int((x + w) * width)\n",
        "        y2 = int((y + h) * height)\n",
        "    draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n",
        "    draw.text((x, y), id2label[class_idx], fill=\"white\")\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh-0qf3iMQMj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def validate_and_fix_bboxes(bboxes, image_width, image_height, min_area=30):\n",
        "\n",
        "    valid_bboxes = []\n",
        "    valid_indices = []\n",
        "\n",
        "    for i, bbox in enumerate(bboxes):\n",
        "        x, y, w, h = bbox\n",
        "\n",
        "        # Ensure bbox is within image bounds\n",
        "        x = max(0, min(x, image_width - 1))\n",
        "        y = max(0, min(y, image_height - 1))\n",
        "\n",
        "        # Ensure width and height are positive and within bounds\n",
        "        w = max(1, min(w, image_width - x))\n",
        "        h = max(1, min(h, image_height - y))\n",
        "\n",
        "        # Check if bbox has minimum area\n",
        "        if w * h >= min_area:\n",
        "            valid_bboxes.append([x, y, w, h])\n",
        "            valid_indices.append(i)\n",
        "\n",
        "    return valid_bboxes, valid_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wSf9ZhsNDDw"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "\n",
        "train_augment_and_transform = A.Compose(\n",
        "    [\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.5, brightness_limit=0.2, contrast_limit=0.2),\n",
        "        A.HueSaturationValue(p=0.1, hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=20),\n",
        "\n",
        "    ],\n",
        "    bbox_params=A.BboxParams(\n",
        "        format=\"coco\",\n",
        "        label_fields=[\"category\"],\n",
        "        clip=True,\n",
        "        min_area=30,\n",
        "        min_visibility=0.3  # Keep bboxes that are at least 30% visible\n",
        "    ),\n",
        ")\n",
        "\n",
        "validation_transform = A.Compose(\n",
        "    [A.NoOp()],\n",
        "    bbox_params=A.BboxParams(\n",
        "        format=\"coco\",\n",
        "        label_fields=[\"category\"],\n",
        "        clip=True,\n",
        "        min_area=30\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiFpmz23MQPE"
      },
      "outputs": [],
      "source": [
        "def format_image_annotations_as_coco(image_id, categories, areas, bboxes):\n",
        "\n",
        "    annotations = []\n",
        "    for category, area, bbox in zip(categories, areas, bboxes):\n",
        "        formatted_annotation = {\n",
        "            \"image_id\": image_id,\n",
        "            \"category_id\": category,\n",
        "            \"iscrowd\": 0,\n",
        "            \"area\": area,\n",
        "            \"bbox\": list(bbox),\n",
        "        }\n",
        "        annotations.append(formatted_annotation)\n",
        "\n",
        "    return {\n",
        "        \"image_id\": image_id,\n",
        "        \"annotations\": annotations,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTs3DGCYMQRm"
      },
      "outputs": [],
      "source": [
        "def augment_and_transform_batch(examples, transform, image_processor, return_pixel_mask=False):\n",
        "    \"\"\"Apply augmentations and format annotations in COCO format for object detection task\"\"\"\n",
        "\n",
        "    images = []\n",
        "    annotations = []\n",
        "\n",
        "    for image_id, image, objects in zip(examples[\"image_id\"], examples[\"image\"], examples[\"objects\"]):\n",
        "        image = np.array(image.convert(\"RGB\"))\n",
        "        image_height, image_width = image.shape[:2]\n",
        "\n",
        "        # Validate input bounding boxes first\n",
        "        valid_bboxes, valid_indices = validate_and_fix_bboxes(\n",
        "            objects[\"bbox\"], image_width, image_height\n",
        "        )\n",
        "\n",
        "        if not valid_bboxes:\n",
        "            # Skip this image if no valid bboxes\n",
        "            continue\n",
        "\n",
        "        # Filter corresponding categories and areas\n",
        "        valid_categories = [objects[\"category\"][i] for i in valid_indices]\n",
        "        valid_areas = [objects[\"area\"][i] for i in valid_indices]\n",
        "\n",
        "        try:\n",
        "            # Apply augmentations\n",
        "            output = transform(\n",
        "                image=image,\n",
        "                bboxes=valid_bboxes,\n",
        "                category=valid_categories\n",
        "            )\n",
        "\n",
        "            # Validate augmented bboxes\n",
        "            final_bboxes, final_indices = validate_and_fix_bboxes(\n",
        "                output[\"bboxes\"], image_width, image_height\n",
        "            )\n",
        "\n",
        "            if not final_bboxes:\n",
        "                # Skip this image if no valid bboxes after augmentation\n",
        "                continue\n",
        "\n",
        "            final_categories = [output[\"category\"][i] for i in final_indices]\n",
        "            final_areas = [valid_areas[i] for i in final_indices]\n",
        "\n",
        "            images.append(output[\"image\"])\n",
        "\n",
        "            # Format annotations in COCO format\n",
        "            formatted_annotations = format_image_annotations_as_coco(\n",
        "                image_id, final_categories, final_areas, final_bboxes\n",
        "            )\n",
        "            annotations.append(formatted_annotations)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not images:\n",
        "        return {\"pixel_values\": [], \"labels\": []}\n",
        "\n",
        "    result = image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
        "\n",
        "    if not return_pixel_mask:\n",
        "        result.pop(\"pixel_mask\", None)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkFT-OSINNTt"
      },
      "outputs": [],
      "source": [
        "train_transform_batch = partial(\n",
        "    augment_and_transform_batch,\n",
        "    transform=train_augment_and_transform,\n",
        "    image_processor=image_processor\n",
        ")\n",
        "\n",
        "validation_transform_batch = partial(\n",
        "    augment_and_transform_batch,\n",
        "    transform=validation_transform,\n",
        "    image_processor=image_processor\n",
        ")\n",
        "\n",
        "cppe5[\"train\"] = cppe5[\"train\"].with_transform(train_transform_batch)\n",
        "cppe5[\"validation\"] = cppe5[\"validation\"].with_transform(validation_transform_batch)\n",
        "cppe5[\"test\"] = cppe5[\"test\"].with_transform(validation_transform_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IroLT0NfNNWY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "    data = {}\n",
        "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
        "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
        "    if \"pixel_mask\" in batch[0]:\n",
        "        data[\"pixel_mask\"] = torch.stack([x[\"pixel_mask\"] for x in batch])\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8Bgz6HxNNcT"
      },
      "outputs": [],
      "source": [
        "from transformers.image_transforms import center_to_corners_format\n",
        "\n",
        "def convert_bbox_yolo_to_pascal(boxes, image_size):\n",
        "\n",
        "\n",
        "    boxes = center_to_corners_format(boxes)\n",
        "    height, width = image_size\n",
        "    boxes = boxes * torch.tensor([[width, height, width, height]])\n",
        "\n",
        "    return boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dQcWewZNNel"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelOutput:\n",
        "    logits: torch.Tensor\n",
        "    pred_boxes: torch.Tensor\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_metrics(evaluation_results, image_processor, threshold=0.0, id2label=None):\n",
        "\n",
        "\n",
        "    predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
        "\n",
        "    image_sizes = []\n",
        "    post_processed_targets = []\n",
        "    post_processed_predictions = []\n",
        "\n",
        "    for batch in targets:\n",
        "        batch_image_sizes = torch.tensor(np.array([x[\"orig_size\"] for x in batch]))\n",
        "        image_sizes.append(batch_image_sizes)\n",
        "\n",
        "\n",
        "        for image_target in batch:\n",
        "            boxes = torch.tensor(image_target[\"boxes\"])\n",
        "            boxes = convert_bbox_yolo_to_pascal(boxes, image_target[\"orig_size\"])\n",
        "            labels = torch.tensor(image_target[\"class_labels\"])\n",
        "            post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
        "\n",
        "\n",
        "    for batch, target_sizes in zip(predictions, image_sizes):\n",
        "        batch_logits, batch_boxes = batch[1], batch[2]\n",
        "        output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n",
        "        post_processed_output = image_processor.post_process_object_detection(\n",
        "            output, threshold=threshold, target_sizes=target_sizes\n",
        "        )\n",
        "        post_processed_predictions.extend(post_processed_output)\n",
        "\n",
        "    # Compute metrics\n",
        "    metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
        "    metric.update(post_processed_predictions, post_processed_targets)\n",
        "    metrics = metric.compute()\n",
        "\n",
        "    classes = metrics.pop(\"classes\")\n",
        "    map_per_class = metrics.pop(\"map_per_class\")\n",
        "    mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
        "    for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
        "        class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
        "        metrics[f\"map_{class_name}\"] = class_map\n",
        "        metrics[f\"mar_100_{class_name}\"] = class_mar\n",
        "\n",
        "    metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "eval_compute_metrics_fn = partial(\n",
        "    compute_metrics, image_processor=image_processor, id2label=id2label, threshold=0.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdgG-6Z0NNh1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForObjectDetection\n",
        "\n",
        "model = AutoModelForObjectDetection.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg9hWmFdNdnu"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"detr_finetuned_cppe5\",\n",
        "    num_train_epochs=30,\n",
        "    fp16=False,\n",
        "    per_device_train_batch_size=8,\n",
        "    dataloader_num_workers=4,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=1e-4,\n",
        "    max_grad_norm=0.01,\n",
        "    metric_for_best_model=\"eval_map\",\n",
        "    greater_is_better=True,\n",
        "    load_best_model_at_end=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    eval_do_concat_batches=False,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9lOmX9vNdp9"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=cppe5[\"train\"],\n",
        "    eval_dataset=cppe5[\"validation\"],\n",
        "    processing_class=image_processor,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=eval_compute_metrics_fn,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSG9lO0CNdso"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "metrics = trainer.evaluate(eval_dataset=cppe5[\"test\"], metric_key_prefix=\"test\")\n",
        "pprint(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY11Iwd9NdvL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
        "\n",
        "url = \"https://images.pexels.com/photos/8413299/pexels-photo-8413299.jpeg?auto=compress&cs=tinysrgb&w=630&h=375&dpr=2\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgK56RkUNdxr"
      },
      "outputs": [],
      "source": [
        "from accelerate.test_utils.testing import get_backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj_N1_t7Nv6l"
      },
      "outputs": [],
      "source": [
        "device, _, _ = get_backend()\n",
        "model_repo = \"qubvel-hf/detr_finetuned_cppe5\"\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_repo)\n",
        "model = AutoModelForObjectDetection.from_pretrained(model_repo)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3RHmXboNv9J"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    inputs = image_processor(images=[image], return_tensors=\"pt\")\n",
        "    outputs = model(**inputs.to(device))\n",
        "    target_sizes = torch.tensor([[image.size[1], image.size[0]]])\n",
        "    results = image_processor.post_process_object_detection(outputs, threshold=0.3, target_sizes=target_sizes)[0]\n",
        "\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(\n",
        "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
        "        f\"{round(score.item(), 3)} at location {box}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cw-ZXCiNv_q"
      },
      "outputs": [],
      "source": [
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    x, y, x2, y2 = tuple(box)\n",
        "    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
        "    draw.text((x, y), model.config.id2label[label.item()], fill=\"white\")\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9kEch26NwCT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZObRR23NwD3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
